{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= pd.read_csv('C:\\\\Users\\\\admin\\\\Documents\\\\Python_Scripts\\\\NP_Binary_COE_contest\\\\ds-coe\\\\Training_Dataset.csv')\n",
    "test_dataset = pd.read_csv('C:\\\\Users\\\\admin\\\\Documents\\\\Python_Scripts\\\\NP_Binary_COE_contest\\\\ds-coe\\\\Test_Dataset.csv')\n",
    "sample_submission = pd.read_csv('C:\\\\Users\\\\admin\\\\Documents\\\\Python_Scripts\\\\NP_Binary_COE_contest\\\\ds-coe\\\\Sample_Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#lemma test\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk import bigrams\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>len_of_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18822638</td>\n",
       "      <td>I should have known better, its obviously not a juul. I was just happy to see Oliver Tree tbh</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9f9f1c3f</td>\n",
       "      <td>I am happy for you. No, seriously, I am.</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563242ec</td>\n",
       "      <td>Everyone including an ambassador has freedom of speech. Get an education.</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ceda51ca</td>\n",
       "      <td>Oh really?</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bfb0c83d</td>\n",
       "      <td>Thanks for reply, I'll enjoy reading through the comments... maybe you make a book of it all.</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24304</th>\n",
       "      <td>2bf61e0d</td>\n",
       "      <td>Suffocation, no breathing</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24305</th>\n",
       "      <td>6cadee8d</td>\n",
       "      <td>They both do! Like wow!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24306</th>\n",
       "      <td>57eaa4da</td>\n",
       "      <td>I also posted about this a few days ago. Maybe they have different designers but none of them has follow-up or approval from a single person, unfortunate</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24307</th>\n",
       "      <td>f1c2acd8</td>\n",
       "      <td>Look into BRI countries, pretty strong overlap with them would be my guess.</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24308</th>\n",
       "      <td>2e209406</td>\n",
       "      <td>James Thomas looks great</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24309 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  \\\n",
       "0      18822638   \n",
       "1      9f9f1c3f   \n",
       "2      563242ec   \n",
       "3      ceda51ca   \n",
       "4      bfb0c83d   \n",
       "...         ...   \n",
       "24304  2bf61e0d   \n",
       "24305  6cadee8d   \n",
       "24306  57eaa4da   \n",
       "24307  f1c2acd8   \n",
       "24308  2e209406   \n",
       "\n",
       "                                                                                                                                                            text  \\\n",
       "0                                                                 I should have known better, its obviously not a juul. I was just happy to see Oliver Tree tbh    \n",
       "1                                                                                                                       I am happy for you. No, seriously, I am.   \n",
       "2                                                                                      Everyone including an ambassador has freedom of speech. Get an education.   \n",
       "3                                                                                                                                                     Oh really?   \n",
       "4                                                                 Thanks for reply, I'll enjoy reading through the comments... maybe you make a book of it all.    \n",
       "...                                                                                                                                                          ...   \n",
       "24304                                                                                                                                  Suffocation, no breathing   \n",
       "24305                                                                                                                                  They both do! Like wow!!!   \n",
       "24306  I also posted about this a few days ago. Maybe they have different designers but none of them has follow-up or approval from a single person, unfortunate   \n",
       "24307                                                                               Look into BRI countries, pretty strong overlap with them would be my guess.    \n",
       "24308                                                                                                                                   James Thomas looks great   \n",
       "\n",
       "       label  len_of_text  \n",
       "0          0           94  \n",
       "1          0           40  \n",
       "2          0           73  \n",
       "3          0           10  \n",
       "4          0           94  \n",
       "...      ...          ...  \n",
       "24304      0           25  \n",
       "24305      0           25  \n",
       "24306      1          153  \n",
       "24307      0           76  \n",
       "24308      0           24  \n",
       "\n",
       "[24309 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Understanding the dataset\n",
    "input_df = train_dataset.copy()\n",
    "input_df['len_of_text'] = input_df['text'].astype(str).map(len)\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24309 entries, 0 to 24308\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           24309 non-null  object\n",
      " 1   text         24309 non-null  object\n",
      " 2   label        24309 non-null  int64 \n",
      " 3   len_of_text  24309 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 759.8+ KB\n"
     ]
    }
   ],
   "source": [
    "input_df.info() #no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>len_of_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24309.000000</td>\n",
       "      <td>24309.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.210580</td>\n",
       "      <td>69.697561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.407729</td>\n",
       "      <td>37.455002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>703.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label   len_of_text\n",
       "count  24309.000000  24309.000000\n",
       "mean       0.210580     69.697561\n",
       "std        0.407729     37.455002\n",
       "min        0.000000      2.000000\n",
       "25%        0.000000     39.000000\n",
       "50%        0.000000     66.000000\n",
       "75%        0.000000     97.000000\n",
       "max        1.000000    703.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.describe() #percentile distribuion of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>len_of_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_of_text</th>\n",
       "      <td>0.042918</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                label  len_of_text\n",
       "label        1.000000     0.042918\n",
       "len_of_text  0.042918     1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.corr() #as expected no relation to the len of text and the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ID    24309\n",
       " dtype: int64,\n",
       " ID    5119\n",
       " dtype: int64,\n",
       " ID    19190\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.count().head(1), input_df[input_df['label']==1].count().head(1), input_df[input_df['label']==0].count().head(1)\n",
    "#4:1 class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making\n",
      "make\n",
      "mak\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize Single Word\n",
    "print(lemmatizer.lemmatize(\"making\"))\n",
    "print(lemmatizer.lemmatize(\"make\"))\n",
    "print(lemmatizer.lemmatize(\"mak\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into test and train\n",
    "train_cp_df, test_cp_df = train_test_split(train_dataset,test_size = 0.1)\n",
    "train_df1 = train_cp_df.copy()\n",
    "test_df = test_cp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ID    21878\n",
       " dtype: int64,\n",
       " ID    4585\n",
       " dtype: int64,\n",
       " ID    17293\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1.count().head(1), train_df1[train_df1['label']==1].count().head(1), train_df1[train_df1['label']==0].count().head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-254-f260f19e6257>:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  test_df.count().head(1), test_df[input_df['label']==1].count().head(1), test_df[input_df['label']==0].count().head(1)\n",
      "<ipython-input-254-f260f19e6257>:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  test_df.count().head(1), test_df[input_df['label']==1].count().head(1), test_df[input_df['label']==0].count().head(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ID    2431\n",
       " dtype: int64,\n",
       " ID    534\n",
       " dtype: int64,\n",
       " ID    1897\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count().head(1), test_df[input_df['label']==1].count().head(1), test_df[input_df['label']==0].count().head(1)\n",
    "#similar split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df1.copy() #no oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ID    31048\n",
       " dtype: int64,\n",
       " ID    13755\n",
       " dtype: int64,\n",
       " ID    17293\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oversampling - twice\n",
    "lbl_df = train_df1[train_df1['label']==1]\n",
    "lbl1_df = lbl_df.copy()\n",
    "lbl1_df['ID'] = lbl1_df['ID']+'xx1'\n",
    "lbl2_df = lbl_df.copy()\n",
    "lbl2_df['ID'] = lbl2_df['ID']+'xx2'\n",
    "train_df2 = pd.concat([train_df1,lbl1_df])\n",
    "train_df = pd.concat([train_df2,lbl2_df])\n",
    "train_df.count().head(1), train_df[train_df['label']==1].count().head(1), train_df[train_df['label']==0].count().head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "train_df['text_stop'] = train_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-035fe6f80936>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['text_wtkn_stop'] = train_df['text'].apply(lambda x: word_tokenize(x))\n",
      "<ipython-input-61-035fe6f80936>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['text_stkn_stop'] = train_df['text'].apply(lambda x: sent_tokenize(x))\n"
     ]
    }
   ],
   "source": [
    "#word tokenize\n",
    "#train_df['text_wtkn_stop'] = train_df['text'].apply(lambda x: word_tokenize(x))\n",
    "#train_df['text_stkn_stop'] = train_df['text'].apply(lambda x: sent_tokenize(x))\n",
    "#train_df['text_stkn_stop'] = train_df['text'].apply(sent_tokenize).apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>b8feec0e</td>\n",
       "      <td>I couldn't let Christina Payne become carver.....</td>\n",
       "      <td>1</td>\n",
       "      <td>I let Christina Payne become carver.. I regret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8707</th>\n",
       "      <td>4e0109d4</td>\n",
       "      <td>Why would the landlord pay it if electric isnt...</td>\n",
       "      <td>1</td>\n",
       "      <td>Why would landlord pay electric isnt included....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10891</th>\n",
       "      <td>4d84fbd6</td>\n",
       "      <td>You‚Äôre right, 5 minutes won‚Äôt be enough, earli...</td>\n",
       "      <td>0</td>\n",
       "      <td>You‚Äôre right, 5 minutes won‚Äôt enough, earlier ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15531</th>\n",
       "      <td>42a63924</td>\n",
       "      <td>...and his last one was in New Orleans. Let‚Äôs ...</td>\n",
       "      <td>0</td>\n",
       "      <td>...and last one New Orleans. Let‚Äôs go Rams!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>f5878f9c</td>\n",
       "      <td>&gt; mined the worldwide largest block It was orp...</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; mined worldwide largest block It orphaned, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13475</th>\n",
       "      <td>1a875d4b</td>\n",
       "      <td>Canadian Kristi Wolfe here to disagree with yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Canadian Kristi Wolfe disagree you. Politely. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15110</th>\n",
       "      <td>03eb2fc2</td>\n",
       "      <td>It's true though. He either gets no shirt and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>It's true though. He either gets shirt freezes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21388</th>\n",
       "      <td>2dd3be45</td>\n",
       "      <td>Wow he is very gifted! I hope he continues to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wow gifted! I hope continues freeze buffalos!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23622</th>\n",
       "      <td>cf45303a</td>\n",
       "      <td>Keep them until November, and then return them.</td>\n",
       "      <td>0</td>\n",
       "      <td>Keep November, return them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17680</th>\n",
       "      <td>de66f0a7</td>\n",
       "      <td>Were you looking for a tall person</td>\n",
       "      <td>0</td>\n",
       "      <td>Were looking tall person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21878 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               text  label  \\\n",
       "3868   b8feec0e  I couldn't let Christina Payne become carver.....      1   \n",
       "8707   4e0109d4  Why would the landlord pay it if electric isnt...      1   \n",
       "10891  4d84fbd6  You‚Äôre right, 5 minutes won‚Äôt be enough, earli...      0   \n",
       "15531  42a63924  ...and his last one was in New Orleans. Let‚Äôs ...      0   \n",
       "5526   f5878f9c  > mined the worldwide largest block It was orp...      0   \n",
       "...         ...                                                ...    ...   \n",
       "13475  1a875d4b  Canadian Kristi Wolfe here to disagree with yo...      1   \n",
       "15110  03eb2fc2  It's true though. He either gets no shirt and ...      0   \n",
       "21388  2dd3be45  Wow he is very gifted! I hope he continues to ...      0   \n",
       "23622  cf45303a   Keep them until November, and then return them.       0   \n",
       "17680  de66f0a7                 Were you looking for a tall person      0   \n",
       "\n",
       "                                               text_stop  \n",
       "3868   I let Christina Payne become carver.. I regret...  \n",
       "8707   Why would landlord pay electric isnt included....  \n",
       "10891  You‚Äôre right, 5 minutes won‚Äôt enough, earlier ...  \n",
       "15531        ...and last one New Orleans. Let‚Äôs go Rams!  \n",
       "5526   > mined worldwide largest block It orphaned, u...  \n",
       "...                                                  ...  \n",
       "13475  Canadian Kristi Wolfe disagree you. Politely. ...  \n",
       "15110  It's true though. He either gets shirt freezes...  \n",
       "21388      Wow gifted! I hope continues freeze buffalos!  \n",
       "23622                        Keep November, return them.  \n",
       "17680                           Were looking tall person  \n",
       "\n",
       "[21878 rows x 4 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                                                                                          053c9566\n",
       "text          But perhaps it opens a door to your escape from a lifetime of dealing with that. I would call it a fortuitous misstep.\n",
       "label                                                                                                                              0\n",
       "text_stop                                      But perhaps opens door escape lifetime dealing that. I would call fortuitous misstep.\n",
       "emoji_text                                     But perhaps opens door escape lifetime dealing that. I would call fortuitous misstep.\n",
       "lemma_text                                      But perhaps open door escape lifetime dealing that. I would call fortuitous misstep.\n",
       "Name: 17155, dtype: object"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[3239]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "train_df['emoji_text'] = train_df['text_stop'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_stop</th>\n",
       "      <th>emoji_text</th>\n",
       "      <th>lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13423</th>\n",
       "      <td>bcec4c5a</td>\n",
       "      <td>Don't forget the price increase. They made sur...</td>\n",
       "      <td>0</td>\n",
       "      <td>Don't forget price increase. They made sure ke...</td>\n",
       "      <td>Don't forget price increase. They made sure ke...</td>\n",
       "      <td>[Don't, forget, price, increase., They, made, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841</th>\n",
       "      <td>86995857</td>\n",
       "      <td>Hahahahahaha you 100 percent got my ass first ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hahahahahaha 100 percent got ass first time mo...</td>\n",
       "      <td>Hahahahahaha 100 percent got ass first time mo...</td>\n",
       "      <td>[Hahahahahaha, 100, percent, got, as, first, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22402</th>\n",
       "      <td>ecdfbc63</td>\n",
       "      <td>The saddest part is that just this morning whe...</td>\n",
       "      <td>0</td>\n",
       "      <td>The saddest part morning laying bed I thinking...</td>\n",
       "      <td>The saddest part morning laying bed I thinking...</td>\n",
       "      <td>[The, saddest, part, morning, laying, bed, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24146</th>\n",
       "      <td>72bcf333</td>\n",
       "      <td>If your walls are not connected sprinkle your ...</td>\n",
       "      <td>0</td>\n",
       "      <td>If walls connected sprinkle lawn regularly dia...</td>\n",
       "      <td>If walls connected sprinkle lawn regularly dia...</td>\n",
       "      <td>[If, wall, connected, sprinkle, lawn, regularl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>890282df</td>\n",
       "      <td>Except they miss most of those too.</td>\n",
       "      <td>0</td>\n",
       "      <td>Except miss too.</td>\n",
       "      <td>Except miss too.</td>\n",
       "      <td>[Except, miss, too.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23287</th>\n",
       "      <td>976acd7e</td>\n",
       "      <td>Luckily I don‚Äôt like beer so I don‚Äôt have to w...</td>\n",
       "      <td>0</td>\n",
       "      <td>Luckily I don‚Äôt like beer I don‚Äôt worry that.....</td>\n",
       "      <td>Luckily I don‚Äôt like beer I don‚Äôt worry that.....</td>\n",
       "      <td>[Luckily, I, don‚Äôt, like, beer, I, don‚Äôt, worr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14740</th>\n",
       "      <td>df383fc4</td>\n",
       "      <td>My girlfriend ordered Chipotle. Have fun with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>My girlfriend ordered Chipotle. Have fun 15 mi...</td>\n",
       "      <td>My girlfriend ordered Chipotle. Have fun 15 mi...</td>\n",
       "      <td>[My, girlfriend, ordered, Chipotle., Have, fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>f3fbc8d0</td>\n",
       "      <td>What exactly is the point of stating something...</td>\n",
       "      <td>0</td>\n",
       "      <td>What exactly point stating something ‚Äúright al...</td>\n",
       "      <td>What exactly point stating something ‚Äúright al...</td>\n",
       "      <td>[What, exactly, point, stating, something, ‚Äúri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21902</th>\n",
       "      <td>9fede820</td>\n",
       "      <td>Nah I think she probably knows he will side wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I think probably knows side mom lol</td>\n",
       "      <td>Nah I think probably knows side mom lol</td>\n",
       "      <td>[Nah, I, think, probably, know, side, mom, lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12631</th>\n",
       "      <td>16cebfd4</td>\n",
       "      <td>I can't believe he told that story. I'm sure t...</td>\n",
       "      <td>0</td>\n",
       "      <td>I can't believe told story. I'm sure minister ...</td>\n",
       "      <td>I can't believe told story. I'm sure minister ...</td>\n",
       "      <td>[I, can't, believe, told, story., I'm, sure, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21878 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                               text  label  \\\n",
       "13423  bcec4c5a  Don't forget the price increase. They made sur...      0   \n",
       "2841   86995857  Hahahahahaha you 100 percent got my ass first ...      0   \n",
       "22402  ecdfbc63  The saddest part is that just this morning whe...      0   \n",
       "24146  72bcf333  If your walls are not connected sprinkle your ...      0   \n",
       "7697   890282df                Except they miss most of those too.      0   \n",
       "...         ...                                                ...    ...   \n",
       "23287  976acd7e  Luckily I don‚Äôt like beer so I don‚Äôt have to w...      0   \n",
       "14740  df383fc4  My girlfriend ordered Chipotle. Have fun with ...      0   \n",
       "1818   f3fbc8d0  What exactly is the point of stating something...      0   \n",
       "21902  9fede820  Nah I think she probably knows he will side wi...      0   \n",
       "12631  16cebfd4  I can't believe he told that story. I'm sure t...      0   \n",
       "\n",
       "                                               text_stop  \\\n",
       "13423  Don't forget price increase. They made sure ke...   \n",
       "2841   Hahahahahaha 100 percent got ass first time mo...   \n",
       "22402  The saddest part morning laying bed I thinking...   \n",
       "24146  If walls connected sprinkle lawn regularly dia...   \n",
       "7697                                    Except miss too.   \n",
       "...                                                  ...   \n",
       "23287  Luckily I don‚Äôt like beer I don‚Äôt worry that.....   \n",
       "14740  My girlfriend ordered Chipotle. Have fun 15 mi...   \n",
       "1818   What exactly point stating something ‚Äúright al...   \n",
       "21902            Nah I think probably knows side mom lol   \n",
       "12631  I can't believe told story. I'm sure minister ...   \n",
       "\n",
       "                                              emoji_text  \\\n",
       "13423  Don't forget price increase. They made sure ke...   \n",
       "2841   Hahahahahaha 100 percent got ass first time mo...   \n",
       "22402  The saddest part morning laying bed I thinking...   \n",
       "24146  If walls connected sprinkle lawn regularly dia...   \n",
       "7697                                    Except miss too.   \n",
       "...                                                  ...   \n",
       "23287  Luckily I don‚Äôt like beer I don‚Äôt worry that.....   \n",
       "14740  My girlfriend ordered Chipotle. Have fun 15 mi...   \n",
       "1818   What exactly point stating something ‚Äúright al...   \n",
       "21902            Nah I think probably knows side mom lol   \n",
       "12631  I can't believe told story. I'm sure minister ...   \n",
       "\n",
       "                                              lemma_text  \n",
       "13423  [Don't, forget, price, increase., They, made, ...  \n",
       "2841   [Hahahahahaha, 100, percent, got, as, first, t...  \n",
       "22402  [The, saddest, part, morning, laying, bed, I, ...  \n",
       "24146  [If, wall, connected, sprinkle, lawn, regularl...  \n",
       "7697                                [Except, miss, too.]  \n",
       "...                                                  ...  \n",
       "23287  [Luckily, I, don‚Äôt, like, beer, I, don‚Äôt, worr...  \n",
       "14740  [My, girlfriend, ordered, Chipotle., Have, fun...  \n",
       "1818   [What, exactly, point, stating, something, ‚Äúri...  \n",
       "21902    [Nah, I, think, probably, know, side, mom, lol]  \n",
       "12631  [I, can't, believe, told, story., I'm, sure, m...  \n",
       "\n",
       "[21878 rows x 6 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_stop</th>\n",
       "      <th>emoji_text</th>\n",
       "      <th>lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14816</th>\n",
       "      <td>17b74dfe</td>\n",
       "      <td>Now that's brilliant... 30? woah. I'm only 19, hope it doesn't take me that long</td>\n",
       "      <td>0</td>\n",
       "      <td>Now that's brilliant... 30? woah. I'm 19, hope take long</td>\n",
       "      <td>Now that's brilliant... 30? woah. I'm 19, hope take long</td>\n",
       "      <td>Now that's brilliant... 30? woah. I'm 19, hope take long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19366</th>\n",
       "      <td>4078234f</td>\n",
       "      <td>Boondock Saints, a brilliant documentary on the South Boston psyche</td>\n",
       "      <td>0</td>\n",
       "      <td>Boondock Saints, brilliant documentary South Boston psyche</td>\n",
       "      <td>Boondock Saints, brilliant documentary South Boston psyche</td>\n",
       "      <td>Boondock Saints, brilliant documentary South Boston psyche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>4192b4a5</td>\n",
       "      <td>Then enjoy your shutdown!</td>\n",
       "      <td>0</td>\n",
       "      <td>Then enjoy shutdown!</td>\n",
       "      <td>Then enjoy shutdown!</td>\n",
       "      <td>Then enjoy shutdown!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20320</th>\n",
       "      <td>2bd5cbc5</td>\n",
       "      <td>No, I prefer having a forest</td>\n",
       "      <td>1</td>\n",
       "      <td>No, I prefer forest</td>\n",
       "      <td>No, I prefer forest</td>\n",
       "      <td>No, I prefer forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>c77e2b23</td>\n",
       "      <td>Came here to say this. I thank you kindly. Long live America. üòÅ</td>\n",
       "      <td>0</td>\n",
       "      <td>Came say this. I thank kindly. Long live America. üòÅ</td>\n",
       "      <td>Came say this. I thank kindly. Long live America.</td>\n",
       "      <td>Came say this. I thank kindly. Long live America.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  \\\n",
       "14816  17b74dfe   \n",
       "19366  4078234f   \n",
       "13603  4192b4a5   \n",
       "20320  2bd5cbc5   \n",
       "965    c77e2b23   \n",
       "\n",
       "                                                                                    text  \\\n",
       "14816  Now that's brilliant... 30? woah. I'm only 19, hope it doesn't take me that long    \n",
       "19366               Boondock Saints, a brilliant documentary on the South Boston psyche    \n",
       "13603                                                          Then enjoy your shutdown!   \n",
       "20320                                                       No, I prefer having a forest   \n",
       "965                      Came here to say this. I thank you kindly. Long live America. üòÅ   \n",
       "\n",
       "       label                                                   text_stop  \\\n",
       "14816      0    Now that's brilliant... 30? woah. I'm 19, hope take long   \n",
       "19366      0  Boondock Saints, brilliant documentary South Boston psyche   \n",
       "13603      0                                        Then enjoy shutdown!   \n",
       "20320      1                                         No, I prefer forest   \n",
       "965        0         Came say this. I thank kindly. Long live America. üòÅ   \n",
       "\n",
       "                                                       emoji_text  \\\n",
       "14816    Now that's brilliant... 30? woah. I'm 19, hope take long   \n",
       "19366  Boondock Saints, brilliant documentary South Boston psyche   \n",
       "13603                                        Then enjoy shutdown!   \n",
       "20320                                         No, I prefer forest   \n",
       "965            Came say this. I thank kindly. Long live America.    \n",
       "\n",
       "                                                       lemma_text  \n",
       "14816    Now that's brilliant... 30? woah. I'm 19, hope take long  \n",
       "19366  Boondock Saints, brilliant documentary South Boston psyche  \n",
       "13603                                        Then enjoy shutdown!  \n",
       "20320                                         No, I prefer forest  \n",
       "965             Came say this. I thank kindly. Long live America.  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatization\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "    return ' '.join(lemma_words)\n",
    "\n",
    "\n",
    "train_df['lemma_text'] = train_df['emoji_text'].apply(lemmatize_text)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stem\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming_text(text):\n",
    "    stem_words = [stemmer.stem(w) for w in w_tokenizer.tokenize(text)]\n",
    "    return ' '.join(stem_words)\n",
    "\n",
    "train_df['stem_text'] = train_df['lemma_text'].apply(stemming_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_stop</th>\n",
       "      <th>emoji_text</th>\n",
       "      <th>lemma_text</th>\n",
       "      <th>stem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13423</th>\n",
       "      <td>bcec4c5a</td>\n",
       "      <td>Don't forget the price increase. They made sure to keep that promise.</td>\n",
       "      <td>0</td>\n",
       "      <td>Don't forget price increase. They made sure keep promise.</td>\n",
       "      <td>Don't forget the price increase. They made sure to keep that promise.</td>\n",
       "      <td>Don't forget the price increase. They made sure to keep that promise.</td>\n",
       "      <td>don't forget the price increase. they made sure to keep that promise.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841</th>\n",
       "      <td>86995857</td>\n",
       "      <td>Hahahahahaha you 100 percent got my ass first time in months I‚Äôve been got. Thank you.</td>\n",
       "      <td>0</td>\n",
       "      <td>Hahahahahaha 100 percent got ass first time months I‚Äôve got. Thank you.</td>\n",
       "      <td>Hahahahahaha you 100 percent got my ass first time in months I‚Äôve been got. Thank you.</td>\n",
       "      <td>Hahahahahaha you 100 percent got my as first time in month I‚Äôve been got. Thank you.</td>\n",
       "      <td>hahahahahaha you 100 percent got my ass first time in month i'v been got. thank you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22402</th>\n",
       "      <td>ecdfbc63</td>\n",
       "      <td>The saddest part is that just this morning when we were laying in bed I was thinking about how much I love him.</td>\n",
       "      <td>0</td>\n",
       "      <td>The saddest part morning laying bed I thinking much I love him.</td>\n",
       "      <td>The saddest part is that just this morning when we were laying in bed I was thinking about how much I love him.</td>\n",
       "      <td>The saddest part is that just this morning when we were laying in bed I wa thinking about how much I love him.</td>\n",
       "      <td>the saddest part is that just this morn when we were lay in bed i was think about how much i love him.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24146</th>\n",
       "      <td>72bcf333</td>\n",
       "      <td>If your walls are not connected sprinkle your lawn in between regularly with diatomaceous earth. It will kill them before they can get to you.</td>\n",
       "      <td>0</td>\n",
       "      <td>If walls connected sprinkle lawn regularly diatomaceous earth. It kill get you.</td>\n",
       "      <td>If your walls are not connected sprinkle your lawn in between regularly with diatomaceous earth. It will kill them before they can get to you.</td>\n",
       "      <td>If your wall are not connected sprinkle your lawn in between regularly with diatomaceous earth. It will kill them before they can get to you.</td>\n",
       "      <td>if your wall are not connect sprinkl your lawn in between regular with diatomac earth. it will kill them befor they can get to you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>890282df</td>\n",
       "      <td>Except they miss most of those too.</td>\n",
       "      <td>0</td>\n",
       "      <td>Except miss too.</td>\n",
       "      <td>Except they miss most of those too.</td>\n",
       "      <td>Except they miss most of those too.</td>\n",
       "      <td>except they miss most of those too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23287</th>\n",
       "      <td>976acd7e</td>\n",
       "      <td>Luckily I don‚Äôt like beer so I don‚Äôt have to worry about that.. but my love for bread more than makes up for that!</td>\n",
       "      <td>0</td>\n",
       "      <td>Luckily I don‚Äôt like beer I don‚Äôt worry that.. love bread makes that!</td>\n",
       "      <td>Luckily I don‚Äôt like beer so I don‚Äôt have to worry about that.. but my love for bread more than makes up for that!</td>\n",
       "      <td>Luckily I don‚Äôt like beer so I don‚Äôt have to worry about that.. but my love for bread more than make up for that!</td>\n",
       "      <td>luckili i don't like beer so i don't have to worri about that.. but my love for bread more than make up for that!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14740</th>\n",
       "      <td>df383fc4</td>\n",
       "      <td>My girlfriend ordered Chipotle. Have fun with that 15 mile drive bud.</td>\n",
       "      <td>0</td>\n",
       "      <td>My girlfriend ordered Chipotle. Have fun 15 mile drive bud.</td>\n",
       "      <td>My girlfriend ordered Chipotle. Have fun with that 15 mile drive bud.</td>\n",
       "      <td>My girlfriend ordered Chipotle. Have fun with that 15 mile drive bud.</td>\n",
       "      <td>my girlfriend order chipotle. have fun with that 15 mile drive bud.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>f3fbc8d0</td>\n",
       "      <td>What exactly is the point of stating something as being a ‚Äúright aligned opinion‚Äù then?</td>\n",
       "      <td>0</td>\n",
       "      <td>What exactly point stating something ‚Äúright aligned opinion‚Äù then?</td>\n",
       "      <td>What exactly is the point of stating something as being a ‚Äúright aligned opinion‚Äù then?</td>\n",
       "      <td>What exactly is the point of stating something a being a ‚Äúright aligned opinion‚Äù then?</td>\n",
       "      <td>what exact is the point of state someth as be a ‚Äúright align opinion‚Äù then?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21902</th>\n",
       "      <td>9fede820</td>\n",
       "      <td>Nah I think she probably knows he will side with her mom lol</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I think probably knows side mom lol</td>\n",
       "      <td>Nah I think she probably knows he will side with her mom lol</td>\n",
       "      <td>Nah I think she probably know he will side with her mom lol</td>\n",
       "      <td>nah i think she probabl know he will side with her mom lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12631</th>\n",
       "      <td>16cebfd4</td>\n",
       "      <td>I can't believe he told that story. I'm sure the minister that released the water is now so totally regretting not asking for a bj!</td>\n",
       "      <td>0</td>\n",
       "      <td>I can't believe told story. I'm sure minister released water totally regretting asking bj!</td>\n",
       "      <td>I can't believe he told that story. I'm sure the minister that released the water is now so totally regretting not asking for a bj!</td>\n",
       "      <td>I can't believe he told that story. I'm sure the minister that released the water is now so totally regretting not asking for a bj!</td>\n",
       "      <td>i can't believ he told that story. i'm sure the minist that releas the water is now so total regret not ask for a bj!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21878 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  \\\n",
       "13423  bcec4c5a   \n",
       "2841   86995857   \n",
       "22402  ecdfbc63   \n",
       "24146  72bcf333   \n",
       "7697   890282df   \n",
       "...         ...   \n",
       "23287  976acd7e   \n",
       "14740  df383fc4   \n",
       "1818   f3fbc8d0   \n",
       "21902  9fede820   \n",
       "12631  16cebfd4   \n",
       "\n",
       "                                                                                                                                                 text  \\\n",
       "13423                                                                           Don't forget the price increase. They made sure to keep that promise.   \n",
       "2841                                                           Hahahahahaha you 100 percent got my ass first time in months I‚Äôve been got. Thank you.   \n",
       "22402                                 The saddest part is that just this morning when we were laying in bed I was thinking about how much I love him.   \n",
       "24146  If your walls are not connected sprinkle your lawn in between regularly with diatomaceous earth. It will kill them before they can get to you.   \n",
       "7697                                                                                                              Except they miss most of those too.   \n",
       "...                                                                                                                                               ...   \n",
       "23287                              Luckily I don‚Äôt like beer so I don‚Äôt have to worry about that.. but my love for bread more than makes up for that!   \n",
       "14740                                                                           My girlfriend ordered Chipotle. Have fun with that 15 mile drive bud.   \n",
       "1818                                                          What exactly is the point of stating something as being a ‚Äúright aligned opinion‚Äù then?   \n",
       "21902                                                                                    Nah I think she probably knows he will side with her mom lol   \n",
       "12631             I can't believe he told that story. I'm sure the minister that released the water is now so totally regretting not asking for a bj!   \n",
       "\n",
       "       label  \\\n",
       "13423      0   \n",
       "2841       0   \n",
       "22402      0   \n",
       "24146      0   \n",
       "7697       0   \n",
       "...      ...   \n",
       "23287      0   \n",
       "14740      0   \n",
       "1818       0   \n",
       "21902      0   \n",
       "12631      0   \n",
       "\n",
       "                                                                                        text_stop  \\\n",
       "13423                                   Don't forget price increase. They made sure keep promise.   \n",
       "2841                      Hahahahahaha 100 percent got ass first time months I‚Äôve got. Thank you.   \n",
       "22402                             The saddest part morning laying bed I thinking much I love him.   \n",
       "24146             If walls connected sprinkle lawn regularly diatomaceous earth. It kill get you.   \n",
       "7697                                                                             Except miss too.   \n",
       "...                                                                                           ...   \n",
       "23287                       Luckily I don‚Äôt like beer I don‚Äôt worry that.. love bread makes that!   \n",
       "14740                                 My girlfriend ordered Chipotle. Have fun 15 mile drive bud.   \n",
       "1818                           What exactly point stating something ‚Äúright aligned opinion‚Äù then?   \n",
       "21902                                                     Nah I think probably knows side mom lol   \n",
       "12631  I can't believe told story. I'm sure minister released water totally regretting asking bj!   \n",
       "\n",
       "                                                                                                                                           emoji_text  \\\n",
       "13423                                                                           Don't forget the price increase. They made sure to keep that promise.   \n",
       "2841                                                           Hahahahahaha you 100 percent got my ass first time in months I‚Äôve been got. Thank you.   \n",
       "22402                                 The saddest part is that just this morning when we were laying in bed I was thinking about how much I love him.   \n",
       "24146  If your walls are not connected sprinkle your lawn in between regularly with diatomaceous earth. It will kill them before they can get to you.   \n",
       "7697                                                                                                              Except they miss most of those too.   \n",
       "...                                                                                                                                               ...   \n",
       "23287                              Luckily I don‚Äôt like beer so I don‚Äôt have to worry about that.. but my love for bread more than makes up for that!   \n",
       "14740                                                                           My girlfriend ordered Chipotle. Have fun with that 15 mile drive bud.   \n",
       "1818                                                          What exactly is the point of stating something as being a ‚Äúright aligned opinion‚Äù then?   \n",
       "21902                                                                                    Nah I think she probably knows he will side with her mom lol   \n",
       "12631             I can't believe he told that story. I'm sure the minister that released the water is now so totally regretting not asking for a bj!   \n",
       "\n",
       "                                                                                                                                          lemma_text  \\\n",
       "13423                                                                          Don't forget the price increase. They made sure to keep that promise.   \n",
       "2841                                                            Hahahahahaha you 100 percent got my as first time in month I‚Äôve been got. Thank you.   \n",
       "22402                                 The saddest part is that just this morning when we were laying in bed I wa thinking about how much I love him.   \n",
       "24146  If your wall are not connected sprinkle your lawn in between regularly with diatomaceous earth. It will kill them before they can get to you.   \n",
       "7697                                                                                                             Except they miss most of those too.   \n",
       "...                                                                                                                                              ...   \n",
       "23287                              Luckily I don‚Äôt like beer so I don‚Äôt have to worry about that.. but my love for bread more than make up for that!   \n",
       "14740                                                                          My girlfriend ordered Chipotle. Have fun with that 15 mile drive bud.   \n",
       "1818                                                          What exactly is the point of stating something a being a ‚Äúright aligned opinion‚Äù then?   \n",
       "21902                                                                                    Nah I think she probably know he will side with her mom lol   \n",
       "12631            I can't believe he told that story. I'm sure the minister that released the water is now so totally regretting not asking for a bj!   \n",
       "\n",
       "                                                                                                                                 stem_text  \n",
       "13423                                                                don't forget the price increase. they made sure to keep that promise.  \n",
       "2841                                                  hahahahahaha you 100 percent got my ass first time in month i'v been got. thank you.  \n",
       "22402                               the saddest part is that just this morn when we were lay in bed i was think about how much i love him.  \n",
       "24146  if your wall are not connect sprinkl your lawn in between regular with diatomac earth. it will kill them befor they can get to you.  \n",
       "7697                                                                                                   except they miss most of those too.  \n",
       "...                                                                                                                                    ...  \n",
       "23287                    luckili i don't like beer so i don't have to worri about that.. but my love for bread more than make up for that!  \n",
       "14740                                                                  my girlfriend order chipotle. have fun with that 15 mile drive bud.  \n",
       "1818                                                           what exact is the point of state someth as be a ‚Äúright align opinion‚Äù then?  \n",
       "21902                                                                           nah i think she probabl know he will side with her mom lol  \n",
       "12631                i can't believ he told that story. i'm sure the minist that releas the water is now so total regret not ask for a bj!  \n",
       "\n",
       "[21878 rows x 7 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Count vectorizer model\n",
    "count_vec = CountVectorizer(ngram_range=(1,3), max_features=2000, strip_accents='unicode')\n",
    "count_vec.fit(train_df['lemma_text'])\n",
    "train_vectorized = count_vec.transform(train_df['lemma_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_mdl = RandomForestClassifier()\n",
    "rnd_mdl.fit(train_vectorized, train_df['label'])\n",
    "#train accuracy\n",
    "y_train_pred = rnd_mdl.predict(train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9855488141202426"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_f1 = f1_score(train_df['label'], y_train_pred)\n",
    "train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#internal test data check\n",
    "test_df['text_stop'] = test_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "test_df['emoji_text'] = test_df['text_stop'].apply(lambda x: remove_emoji(x))\n",
    "test_df['lemma_text'] = test_df['emoji_text'].apply(lemmatize_text)\n",
    "#test_df['stem_text'] = test_df['text'].apply(stemming_text)\n",
    "test_vectorized = count_vec.transform(test_df['lemma_text'])\n",
    "y_test_pred = rnd_mdl.predict(test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4649610678531702"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1 = f1_score(test_df['label'], y_test_pred)\n",
    "test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1741  156]\n",
      " [ 325  209]] 0.8021390374331551 0.5726027397260274 0.3913857677902622 0.4649610678531702 0.4649610678531702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass labels=weighted as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "#no oversampling\n",
    "print(confusion_matrix(test_df['label'], y_test_pred), accuracy_score(test_df['label'], y_test_pred),\n",
    "precision_score(test_df['label'], y_test_pred), recall_score(test_df['label'], y_test_pred),\n",
    "f1_score(test_df['label'], y_test_pred), f1_score(test_df['label'], y_test_pred,'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1645  252]\n",
      " [ 276  258]] 0.7828054298642534 0.5058823529411764 0.48314606741573035 0.49425287356321834 0.49425287356321834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass labels=weighted as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "#twice oversampling\n",
    "print(confusion_matrix(test_df['label'], y_test_pred), accuracy_score(test_df['label'], y_test_pred),\n",
    "precision_score(test_df['label'], y_test_pred), recall_score(test_df['label'], y_test_pred),\n",
    "f1_score(test_df['label'], y_test_pred), f1_score(test_df['label'], y_test_pred,'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1674  223]\n",
      " [ 286  248]] 0.79062114356232 0.5265392781316348 0.46441947565543074 0.4935323383084577 0.4935323383084577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass labels=weighted as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "#twice oversampling\n",
    "print(confusion_matrix(test_df['label'], y_test_pred), accuracy_score(test_df['label'], y_test_pred),\n",
    "precision_score(test_df['label'], y_test_pred), recall_score(test_df['label'], y_test_pred),\n",
    "f1_score(test_df['label'], y_test_pred), f1_score(test_df['label'], y_test_pred,'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00017625, 0.00046396, 0.00025634, ..., 0.00015675, 0.00014385,\n",
       "       0.00037639])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_mdl.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       dtype=int64),\n",
       " 22948    0\n",
       " 7349     0\n",
       " 9133     0\n",
       " 16038    0\n",
       " 6131     1\n",
       " 4547     0\n",
       " 17163    0\n",
       " 16474    0\n",
       " 10028    1\n",
       " 3239     1\n",
       " 6111     0\n",
       " 13784    0\n",
       " 8081     0\n",
       " 15320    0\n",
       " 22467    1\n",
       " 4227     0\n",
       " 2761     0\n",
       " 866      0\n",
       " 11594    0\n",
       " 6013     0\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred[:20], test_df['label'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>284f775a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1dd04583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49e1e335</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8b86a110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59f206bf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  label\n",
       "0  284f775a      0\n",
       "1  1dd04583      0\n",
       "2  49e1e335      0\n",
       "3  8b86a110      0\n",
       "4  59f206bf      0"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#external test data check\n",
    "test_dataset['text_stop'] = test_dataset['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "test_dataset['emoji_text'] = test_dataset['text_stop'].apply(lambda x: remove_emoji(x))\n",
    "test_dataset['lemma_text'] = test_dataset['emoji_text'].apply(lemmatize_text)\n",
    "#test_df['stem_text'] = test_df['text'].apply(stemming_text)\n",
    "test_dataset_vectorized = count_vec.transform(test_dataset['lemma_text'])\n",
    "y_final_pred = rnd_mdl.predict(test_dataset_vectorized)\n",
    "sample_submission['label'] = y_final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    84.3747\n",
       "1    15.6253\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission['label'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"rnd1_ngram_stop_lemma__2oversampl_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9855615562658436"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "nb_mdl = MultinomialNB()\n",
    "nb_mdl.fit(train_vectorized, train_df['label'])\n",
    "#train accuracy\n",
    "y_train_pred = rnd_mdl.predict(train_vectorized)\n",
    "train_f1 = f1_score(train_df['label'], y_train_pred)\n",
    "train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1726  171]\n",
      " [ 307  227]] 0.8033730974907446 0.5703517587939698 0.4250936329588015 0.4871244635193133\n"
     ]
    }
   ],
   "source": [
    "test_vectorized = count_vec.transform(test_df['lemma_text'])\n",
    "y_test_pred = rnd_mdl.predict(test_vectorized)\n",
    "print(confusion_matrix(test_df['label'], y_test_pred), accuracy_score(test_df['label'], y_test_pred),\n",
    "precision_score(test_df['label'], y_test_pred), recall_score(test_df['label'], y_test_pred),\n",
    "f1_score(test_df['label'], y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 16.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random grid + CV + random forest\n",
    "random_grid = {'bootstrap': [True,False],\n",
    " 'max_depth': [10, 20, 30, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600]}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rnd_mdl, param_distributions = random_grid, n_iter = 100, \n",
    "                               cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(train_vectorized, train_df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 600,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': None,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed: 21.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True], 'max_depth': [40, 80, None],\n",
       "                         'max_features': ['auto'], 'min_samples_leaf': [2, 3],\n",
       "                         'min_samples_split': [2, 3, 4],\n",
       "                         'n_estimators': [600, 800, 1000]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [40,80,None],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [2,3],\n",
    "    'min_samples_split': [2,3,4],\n",
    "    'n_estimators': [600, 800, 1000]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator = rnd_mdl, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(train_vectorized, train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 800}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5424354243542435"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hypertuned random forest\n",
    "rnd_mdl_tuned = RandomForestClassifier(max_features='auto', max_depth=None,\n",
    "                                      min_samples_leaf=2, min_samples_split=2, n_estimators=800, random_state=42)\n",
    "rnd_mdl_tuned.fit(train_vectorized, train_df['label'])\n",
    "#train accuracy\n",
    "y_train_pred = rnd_mdl_tuned.predict(train_vectorized)\n",
    "train_f1 = f1_score(train_df['label'], y_train_pred)\n",
    "train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1838   72]\n",
      " [ 362  159]] 0.8214726450020567 0.6883116883116883 0.30518234165067176 0.42287234042553196\n"
     ]
    }
   ],
   "source": [
    "#test_vectorized = count_vec.transform(test_df['lemma_text'])\n",
    "y_test_pred = rnd_mdl_tuned.predict(test_vectorized)\n",
    "print(confusion_matrix(test_df['label'], y_test_pred), accuracy_score(test_df['label'], y_test_pred),\n",
    "precision_score(test_df['label'], y_test_pred), recall_score(test_df['label'], y_test_pred),\n",
    "f1_score(test_df['label'], y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat plain random forest with tf-idf \n",
    "#Count vectorizer model\n",
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfIdfVectorizer.fit(train_df['lemma_text'])\n",
    "train_tdidf = tfIdfVectorizer.transform(train_df['lemma_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9980366492146596"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_mdl2 = RandomForestClassifier()\n",
    "rnd_mdl2.fit(train_tdidf, train_df['label'])\n",
    "#train accuracy\n",
    "y_train_pred = rnd_mdl2.predict(train_tdidf)\n",
    "train_f1 = f1_score(train_df['label'], y_train_pred)\n",
    "train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tdidf = tfIdfVectorizer.transform(test_df['lemma_text'])\n",
    "y_test_pred = rnd_mdl2.predict(test_tdidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1808   89]\n",
      " [ 345  189]] 0.8214726450020567 0.6798561151079137 0.3539325842696629 0.4655172413793103\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_df['label'], y_test_pred), accuracy_score(test_df['label'], y_test_pred),\n",
    "precision_score(test_df['label'], y_test_pred), recall_score(test_df['label'], y_test_pred),\n",
    "f1_score(test_df['label'], y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#external test data check\n",
    "test_dataset['text_stop'] = test_dataset['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "test_dataset['emoji_text'] = test_dataset['text_stop'].apply(lambda x: remove_emoji(x))\n",
    "test_dataset['lemma_text'] = test_dataset['emoji_text'].apply(lemmatize_text)\n",
    "#test_df['stem_text'] = test_df['text'].apply(stemming_text)\n",
    "test_dataset_tfidf = tfIdfVectorizer.transform(test_dataset['lemma_text'])\n",
    "y_final_pred = rnd_mdl2.predict(test_dataset_tfidf)\n",
    "sample_submission['label'] = y_final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    88.031481\n",
       "1    11.968519\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission['label'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"rnd1_ngram_stop_lemma_tf_2oversampl_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
