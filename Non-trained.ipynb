{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd60e6c6-3ffa-4592-b723-5cfa57089c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ad1e137b4b49efa96fd61a3b3d253d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/593 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f9f4b33b3c428bbe7ff0f21bfc2d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faaf543c0a643ac86aaea14bf3fdb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa46d42939324460a920aed8814c71e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6226677e760421d9c406726c782bccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) not-offensive 0.9073\n",
      "2) offensive 0.0927\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='offensive'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "\n",
    "text = \"Good night ðŸ˜Š\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "# text = \"Good night ðŸ˜Š\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6238f60c-fde3-48d5-9fb4-235aa0f3ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sent(text):\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    max_label = labels[ranking[0]]\n",
    "    max_score = scores[ranking[0]]\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = labels[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        if s>max_score:\n",
    "            max_score=s\n",
    "            max_label=l\n",
    "    return max_label\n",
    "#         print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4419ca6-1634-4a8e-a023-947032f2f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def deEmojify(text):\n",
    "  regrex_pattern = re.compile(pattern = \"[\"\n",
    "      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "      u\"\\U00002702-\\U000027B0\"\n",
    "      u\"\\U00002702-\\U000027B0\"\n",
    "      u\"\\U000024C2-\\U0001F251\"\n",
    "      u\"\\U0001f926-\\U0001f937\"\n",
    "      u\"\\U00010000-\\U0010ffff\"\n",
    "      u\"\\u2640-\\u2642\" \n",
    "      u\"\\u2600-\\u2B55\"\n",
    "      u\"\\u200d\"\n",
    "      u\"\\u23cf\"\n",
    "      u\"\\u23e9\"\n",
    "      u\"\\u231a\"\n",
    "      u\"\\ufe0f\"  # dingbats\n",
    "      u\"\\u3030\"\n",
    "                          \"]+\", flags = re.UNICODE)\n",
    "  return regrex_pattern.sub(r'',text)\n",
    "\n",
    "competition_test = pd.read_csv('Test_Dataset.csv')\n",
    "competition_test['text_no_emoji'] = [deEmojify(x) for x in competition_test.text.values]\n",
    "\n",
    "training_data = pd.read_csv('Training_Dataset.csv')\n",
    "training_data['text_no_emoji'] = [deEmojify(x) for x in training_data.text.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d2295b-f828-46b6-9976-dc83aff1659a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-41e75ecceead>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predicted_label_txt'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpredict_sent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_no_emoji'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predicted_label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicted_label_txt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'not-offensive'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predicted_label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'validation_df' is not defined"
     ]
    }
   ],
   "source": [
    "training_data['predicted_label_txt'] = [predict_sent(x) for x in list(training_data['text_no_emoji'].values)]\n",
    "training_data['predicted_label'] = np.where(training_data.predicted_label_txt.values=='not-offensive',0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2e3a2-d83c-49ba-aaab-f68ba4e8b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(training_data['label'], training_data['predicted_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a281f-241f-4a70-95ed-02d0b13c7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[training_data.label!=training_data.predicted_label].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3bedf1-d17b-4929-aab0-d6aa65e991d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_test['predicted_label_txt'] = [predict_sent(x) for x in list(training_data['text_no_emoji'].values)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
